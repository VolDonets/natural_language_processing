{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.loaders import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dense, Dropout, Flatten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.loaders:Downloading wv\n",
      "INFO:nlpia.web:URL too short: wv\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:expanded+normalized file path: C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
      "INFO:nlpia.loaders:remote_size: 1647046227\n",
      "INFO:nlpia.loaders:local_size: 1647046227\n",
      "INFO:nlpia.loaders:retained: C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:normalize_ext.filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:download_unzip.new_filepaths=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.rename_file(source=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.os.rename(source=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "WARNING:nlpia.loaders:download_unzip.filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:downloaded name=wv to filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepaths={'wv': 'C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz'}\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-12-15T15:22:14.184601', 'gensim': '4.2.0', 'python': '3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "word_vectors = get_data('wv', limit=200_000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess_data(filepath_):\n",
    "    positive_path_ = os.path.join(filepath_, 'pos')\n",
    "    negative_path_ = os.path.join(filepath_, 'neg')\n",
    "    pos_label_ = 1\n",
    "    neg_label_ = 0\n",
    "    dataset_ = []\n",
    "\n",
    "    for filename_ in glob.glob(os.path.join(positive_path_, '*.txt')):\n",
    "        try:\n",
    "            with open(filename_, 'r') as f_:\n",
    "                dataset_.append((pos_label_, f_.read()))\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    for filename_ in glob.glob(os.path.join(negative_path_, '*.txt')):\n",
    "        try:\n",
    "            with open(filename_, 'r') as f_:\n",
    "                dataset_.append((neg_label_, f_.read()))\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    shuffle(dataset_)\n",
    "    return dataset_\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize(dataset_):\n",
    "    tokenizer_ = TreebankWordTokenizer()\n",
    "    vectorized_data_ = []\n",
    "    expected_ = []\n",
    "    for sample_ in dataset_:\n",
    "        tokens_ = tokenizer_.tokenize(sample_[1])\n",
    "        sample_vecs_ = []\n",
    "        for token_ in tokens_:\n",
    "            try:\n",
    "                sample_vecs_.append(word_vectors[token_])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized_data_.append(sample_vecs_)\n",
    "        expected_.append(sample_[0])\n",
    "    return vectorized_data_, expected_\n",
    "\n",
    "\n",
    "def pad_trunc(data_, maxlen_):\n",
    "    new_data_ = []\n",
    "    zero_vector_ = [0.0 for _ in range(len(data_[0][0]))]\n",
    "    for sample_ in data_:\n",
    "        if len(sample_) > maxlen_:\n",
    "            temp_ = sample_[:maxlen_]\n",
    "        elif len(sample_) < maxlen_:\n",
    "            temp_ = sample_\n",
    "            additional_elems_ = maxlen_ - len(sample_)\n",
    "            for _ in range(additional_elems_):\n",
    "                temp_.append(zero_vector_)\n",
    "        else:\n",
    "            temp_ = sample_\n",
    "        new_data_.append(temp_)\n",
    "    return new_data_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because I don't have enough memory: I'll crop the dataset:\n",
      "Len new dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocess_data(\"../src/part_6/stanford_sent_analysis_dataset/aclImdb/train\")\n",
    "print(\"Because I don't have enough memory: I'll crop the dataset:\")\n",
    "dataset = dataset[:5000]\n",
    "print('Len new dataset:', len(dataset))\n",
    "vectorized_data, expected = tokenize_and_vectorize(dataset)\n",
    "del dataset\n",
    "del word_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data) * .8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]\n",
    "del expected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total dataset parameters:\n",
      "Dataset size: 5000\n",
      "Training set size: 4000\n",
      "Test set size: 1000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTotal dataset parameters:\")\n",
    "print(\"Dataset size:\", len(vectorized_data))\n",
    "print(\"Training set size:\", len(x_train))\n",
    "print(\"Test set size:\", len(x_test), \"\\n\\n\")\n",
    "del vectorized_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RNN params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "num_neurons = 25\n",
    "maxlen = 400\n",
    "embedding_dims = 300\n",
    "batch_size = 32\n",
    "epochs = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pad + trunkate the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the bidirectional RNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 400, 50)          16300     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 400, 50)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 20001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,301\n",
      "Trainable params: 36,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(units=num_neurons, return_sequences=True),\n",
    "                        input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 72s 542ms/step - loss: 0.6788 - accuracy: 0.6040 - val_loss: 0.7144 - val_accuracy: 0.6070\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 66s 527ms/step - loss: 0.4315 - accuracy: 0.8045 - val_loss: 0.6508 - val_accuracy: 0.7140\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 67s 537ms/step - loss: 0.3169 - accuracy: 0.8625 - val_loss: 0.6044 - val_accuracy: 0.7360\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 67s 534ms/step - loss: 0.2281 - accuracy: 0.9103 - val_loss: 0.6757 - val_accuracy: 0.7200\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 69s 552ms/step - loss: 0.1646 - accuracy: 0.9383 - val_loss: 0.7013 - val_accuracy: 0.7260\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 66s 529ms/step - loss: 0.1202 - accuracy: 0.9607 - val_loss: 0.7502 - val_accuracy: 0.7220\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 66s 533ms/step - loss: 0.0870 - accuracy: 0.9768 - val_loss: 0.8754 - val_accuracy: 0.7380\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 67s 539ms/step - loss: 0.0599 - accuracy: 0.9837 - val_loss: 0.8713 - val_accuracy: 0.7280\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 65s 522ms/step - loss: 0.0396 - accuracy: 0.9915 - val_loss: 1.0565 - val_accuracy: 0.7190\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 67s 539ms/step - loss: 0.0337 - accuracy: 0.9935 - val_loss: 1.1105 - val_accuracy: 0.7130\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x270a8b1dbb0>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
