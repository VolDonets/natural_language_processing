{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:30:02.741131Z",
     "end_time": "2023-04-24T11:30:15.870901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.loaders import get_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.loaders:Downloading wv\n",
      "INFO:nlpia.web:URL too short: wv\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:expanded+normalized file path: C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
      "INFO:nlpia.loaders:remote_size: 1647046227\n",
      "INFO:nlpia.loaders:local_size: 1647046227\n",
      "INFO:nlpia.loaders:retained: C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:normalize_ext.filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:download_unzip.new_filepaths=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.rename_file(source=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.os.rename(source=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "WARNING:nlpia.loaders:download_unzip.filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:downloaded name=wv to filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepaths={'wv': 'C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz'}\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepath=C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\voldo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-04-24T11:30:57.244553', 'gensim': '4.2.0', 'python': '3.9.10 (tags/v3.9.10:f2f3f53, Jan 17 2022, 15:14:21) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "word_vectors = get_data('wv', limit=200_000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:30:24.794649Z",
     "end_time": "2023-04-24T11:30:57.253659Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def preprocess_data(filepath_):\n",
    "    positive_path_ = os.path.join(filepath_, 'pos')\n",
    "    negative_path_ = os.path.join(filepath_, 'neg')\n",
    "    pos_label_ = 1\n",
    "    neg_label_ = 0\n",
    "    dataset_ = []\n",
    "\n",
    "    for filename_ in glob.glob(os.path.join(positive_path_, '*.txt')):\n",
    "        try:\n",
    "            with open(filename_, 'r') as f_:\n",
    "                dataset_.append((pos_label_, f_.read()))\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    for filename_ in glob.glob(os.path.join(negative_path_, '*.txt')):\n",
    "        try:\n",
    "            with open(filename_, 'r') as f_:\n",
    "                dataset_.append((neg_label_, f_.read()))\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "\n",
    "    shuffle(dataset_)\n",
    "    return dataset_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:31:03.217703Z",
     "end_time": "2023-04-24T11:31:03.228158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset_):\n",
    "    tokenizer_ = TreebankWordTokenizer()\n",
    "    vectorized_data_ = []\n",
    "    expected_ = []\n",
    "    for sample_ in dataset_:\n",
    "        tokens_ = tokenizer_.tokenize(sample_[1])\n",
    "        sample_vecs_ = []\n",
    "        for token_ in tokens_:\n",
    "            try:\n",
    "                sample_vecs_.append(word_vectors[token_])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized_data_.append(sample_vecs_)\n",
    "        expected_.append(sample_[0])\n",
    "    return vectorized_data_, expected_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:31:03.839959Z",
     "end_time": "2023-04-24T11:31:03.865129Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def pad_trunc(data_, maxlen_):\n",
    "    new_data_ = []\n",
    "    zero_vector_ = [0.0 for _ in range(len(data_[0][0]))]\n",
    "    for sample_ in data_:\n",
    "        if len(sample_) > maxlen_:\n",
    "            temp_ = sample_[:maxlen_]\n",
    "        elif len(sample_) < maxlen_:\n",
    "            temp_ = sample_\n",
    "            additional_elems_ = maxlen_ - len(sample_)\n",
    "            for _ in range(additional_elems_):\n",
    "                temp_.append(zero_vector_)\n",
    "        else:\n",
    "            temp_ = sample_\n",
    "        new_data_.append(temp_)\n",
    "    return new_data_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:33:02.841679Z",
     "end_time": "2023-04-24T11:33:02.854829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because I don't have enough memory: I'll crop the dataset:\n",
      "Len new dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocess_data(\"../src/part_6/stanford_sent_analysis_dataset/aclImdb/train\")\n",
    "print(\"Because I don't have enough memory: I'll crop the dataset:\")\n",
    "dataset = dataset[:5000]\n",
    "print('Len new dataset:', len(dataset))\n",
    "vectorized_data, expected = tokenize_and_vectorize(dataset)\n",
    "del dataset\n",
    "del word_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:31:09.128623Z",
     "end_time": "2023-04-24T11:32:33.017720Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data) * .8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]\n",
    "del expected"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:32:39.661886Z",
     "end_time": "2023-04-24T11:32:39.676748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Total dataset parameters:\n",
      "Dataset size: 5000\n",
      "Training set size: 4000\n",
      "Test set size: 1000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTotal dataset parameters:\")\n",
    "print(\"Dataset size:\", len(vectorized_data))\n",
    "print(\"Training set size:\", len(x_train))\n",
    "print(\"Test set size:\", len(x_test), \"\\n\\n\")\n",
    "del vectorized_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:32:46.633951Z",
     "end_time": "2023-04-24T11:32:46.649684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# RNN params:\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 5\n",
    "num_neurons = 25"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:32:49.956439Z",
     "end_time": "2023-04-24T11:32:49.973182Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# pad + trunkate the data\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:33:07.650622Z",
     "end_time": "2023-04-24T11:33:25.408551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building the RNN model:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 400, 25)           8150      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 400, 25)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 10001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,151\n",
      "Trainable params: 18,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the RNN\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=num_neurons,\n",
    "                    return_sequences=True,\n",
    "                    input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(\"\\n\\nBuilding the RNN model:\")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:33:34.040752Z",
     "end_time": "2023-04-24T11:33:35.769100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 28s 197ms/step - loss: 0.7008 - accuracy: 0.5540 - val_loss: 0.7061 - val_accuracy: 0.5850\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.4996 - accuracy: 0.7580 - val_loss: 0.5565 - val_accuracy: 0.7160\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.4040 - accuracy: 0.8108 - val_loss: 0.5409 - val_accuracy: 0.7410\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 0.3397 - accuracy: 0.8593 - val_loss: 0.5454 - val_accuracy: 0.7500\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.2800 - accuracy: 0.8823 - val_loss: 0.6071 - val_accuracy: 0.7440\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x22d21d7e670>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:34:24.462937Z",
     "end_time": "2023-04-24T11:36:32.339821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# save model to file:\n",
    "model_structure = model.to_json()\n",
    "with open('./rnn_model.json', 'w') as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights('./rnn_weights.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T11:37:22.308638Z",
     "end_time": "2023-04-24T11:37:22.386184Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
